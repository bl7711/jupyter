{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79eca0c-b6f2-4272-b647-9eb53d899bf2",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start-NIM-RAG.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"rag_nim_endpoints.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"rag_nim_endpoints.ipynb\">1</a>\n",
    "        <a >2</a>\n",
    "        <a href=\"nim_lora_adapter.ipynb\">3</a>\n",
    "        <!-- <a href=\"challenge.ipynb\">4</a> -->\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"nim_lora_adapter.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135f42d-057d-4fc3-a11c-744eb9ad5b3f",
   "metadata": {},
   "source": [
    "# Building RAG With A Localized NIM\n",
    "---\n",
    "\n",
    "This notebook will demonstrate building a Retrieval Augmented Generation (RAG) pipeline using localized NVIDIA Inference Microservice (NIM). The notebook will walk you through setting up your NVIDIA API Key, pulling and deploying a NIM image, and building a RAG application that uses the locally deployed NIM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1ee23-a007-4bdc-ac6b-d1c0fb6e24c6",
   "metadata": {},
   "source": [
    "### Setup NVIDIA API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a4b22-fb12-4f9f-8c53-50a8dfd8a5de",
   "metadata": {},
   "source": [
    "In the previous notebook, we learned how to set up our generated NVIDIA API KEY. As a requirement for this notebook, you must set up the key as enviroment variable `NVIDIA_API_KEY` to pull the NIMs docker images of your choice. If you haven't gotten your key, please visit the NVIDIA NIMs API [homepage](https://build.nvidia.com/explore/discover) and generate your API Key. Please run the cell below, input your NVIDIA API KEY in the display textbox, and press the enter key on your keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969159f2-b726-49eb-b2d2-5ac8d66f85a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "    os.environ[\"NGC_API_KEY\"] = nvapi_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2290fb-fbf4-4414-849d-664025a7e880",
   "metadata": {},
   "source": [
    "Please execute the cell below to ensure that your docker daemon is up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7035d2-e3a1-41c9-be4f-64310df647bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                      COMMAND                  CREATED        STATUS        PORTS                                                                             NAMES\n"
     ]
    }
   ],
   "source": [
    "! docker ps | egrep \"^CONTAINER ID|nim\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f252ef8-c121-440b-96f4-bed4521bf14a",
   "metadata": {},
   "source": [
    "**Expected Output (if you have no running containers):**\n",
    "\n",
    "```python\n",
    "\n",
    "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501f84f-5e20-4410-ad7f-56ca90284d21",
   "metadata": {},
   "source": [
    "### Login to NVCR (NVIDIA Container Registry)\n",
    "\n",
    "To access a NIM docker image, you must login via `docker login nvcr.io.` This process requires a default username as `--username $oauthtoken` and `--password-stdin` that accepts the value of `$NGC_API_KEY.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20d58181-7190-4a26-950a-aa20ce60d43b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "! echo -e \"$NGC_API_KEY\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06214ad2-8965-4fd4-9d49-9ba6f017227a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "! echo -e \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d2657-8abf-4097-978f-e3991bcd6f41",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "WARNING! Your password will be stored unencrypted in /home/yagupta/.docker/config.json.\n",
    "Configure a credential helper to remove this warning. See\n",
    "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
    "\n",
    "Login Succeeded\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb24f5d-95f9-4ee0-9e37-abc9c4cbbee4",
   "metadata": {},
   "source": [
    "### Pull The Image \n",
    "\n",
    "The next step is to Pull the docker image. We demonstrate this step by pulling `llama3-8b-instruct:1.0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "924eed1e-4420-488e-9fd7-80d4e5b91723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0: Pulling from nim/meta/llama3-8b-instruct\n",
      "Digest: sha256:7fe6071923b547edd9fba87c891a362ea0b4a88794b8a422d63127e54caa6ef7\n",
      "Status: Image is up to date for nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n",
      "nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n"
     ]
    }
   ],
   "source": [
    "! docker pull nvcr.io/nim/meta/llama3-8b-instruct:1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13147b53-61da-40b1-86e8-9f8f82d702c7",
   "metadata": {},
   "source": [
    "**Likely output:** (When you have the image pulled already)\n",
    "```python\n",
    "\n",
    "1.0.0: Pulling from nim/meta/llama3-8b-instruct\n",
    "Digest: sha256:7fe6071923b547edd9fba87c891a362ea0b4a88794b8a422d63127e54caa6ef7\n",
    "Status: Image is up to date for nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n",
    "nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341de29-4a8c-40d5-ba68-82ce6cc7752e",
   "metadata": {},
   "source": [
    "### Pull The Image \n",
    "\n",
    "The next step is to Pull the docker image. We demonstrate this step by pulling `nv-embedqa-e5-v5:1.0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945680b0-86fa-40a2-ae24-966c54c3fe3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1: Pulling from nim/nvidia/nv-embedqa-e5-v5\n",
      "Digest: sha256:128c31a60c4200f02059cb90a8aad0200fcd05fa76700cbf99167de5619c6a46\n",
      "Status: Image is up to date for nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1\n",
      "nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1\n"
     ]
    }
   ],
   "source": [
    "! docker pull nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9068a4b-1f06-4f35-96a3-576d76f86025",
   "metadata": {},
   "source": [
    "**Likely output:** (When you have the image pulled already)\n",
    "```python\n",
    "\n",
    "1.0.1: Pulling from nim/nvidia/nv-embedqa-e5-v5\n",
    "Digest: sha256:128c31a60c4200f02059cb90a8aad0200fcd05fa76700cbf99167de5619c6a46\n",
    "Status: Image is up to date for nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1\n",
    "nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df9235-a28f-474d-b3c4-a597a8b84283",
   "metadata": {},
   "source": [
    "Let's check the model image by listing available images. *Please note that the `IMAGE ID` may differ from what you see under the expected output below*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08697e63-f700-42fd-a916-af43d6365b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                            TAG                       IMAGE ID       CREATED         SIZE\n",
      "nvcr.io/nim/nvidia/nv-embedqa-e5-v5   1.0.1                     fa5c1fc5ccb3   4 months ago    15.7GB\n",
      "nvcr.io/nim/meta/llama3-8b-instruct   1.0.0                     3cb29b0d79e6   5 months ago    12.5GB\n"
     ]
    }
   ],
   "source": [
    "! docker image ls | egrep \"^REPOSITORY|nim\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62c8fc-b3bd-49fe-b0f0-903cc2ca8f4c",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```python\n",
    "REPOSITORY                            TAG                       IMAGE ID       CREATED         SIZE\n",
    "nvcr.io/nim/nvidia/nv-embedqa-e5-v5   1.0.1                     fa5c1fc5ccb3   3 months ago    15.7GB\n",
    "nvcr.io/nim/meta/llama3-8b-instruct   1.0.0                     3cb29b0d79e6   5 months ago    12.5GB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7ac7f-101d-4a35-bbcc-19f550440b5c",
   "metadata": {},
   "source": [
    "#### Setting up Cache for the Model Artifacts\n",
    "\n",
    "The NIMs download a number of files for ensuring the best profiles are selected to achieve max performance on hardware. Set up location for caching the model artifacts as `LOCAL_NIM_CACHE` and export the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b228f8-8733-452a-b096-a205286e68b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/.cache/nim\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "os.environ['LOCAL_NIM_CACHE']=f\"/local/.cache/nim\"\n",
    "!echo $LOCAL_NIM_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3efb68-4a4c-4ae5-a716-efad00a2ee5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "!chmod 755 \"$LOCAL_NIM_CACHE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5e9f5d7-4ed6-43a1-a264-3f6126990164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your have been alloted the available port for llm: 11777\n",
      "Your have been alloted the available port for embeddings: 11807\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import socket\n",
    "\n",
    "def find_available_port(start=11000, end=11999):\n",
    "    while True:\n",
    "        # Randomly select a port between start and end range\n",
    "        port = random.randint(start, end)\n",
    "        \n",
    "        # Try to create a socket and bind to the port\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            try:\n",
    "                sock.bind((\"localhost\", port))\n",
    "                # If binding is successful, the port is free\n",
    "                return port\n",
    "            except OSError:\n",
    "                # If binding fails, the port is in use, continue to the next iteration\n",
    "                continue\n",
    "\n",
    "# Find and print an available port\n",
    "os.environ['LLM_CONTAINER_PORT'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port for llm: {os.environ['LLM_CONTAINER_PORT']}\")\n",
    "\n",
    "os.environ['EMBED_CONTAINER_PORT'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port for embeddings: {os.environ['EMBED_CONTAINER_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351e316d-021d-49dd-bf05-3a5371f7bd80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7321e9c478f315eeff2adcf22e76e76ed614bbe819f14b5b1319d63d43414cad\n"
     ]
    }
   ],
   "source": [
    "! docker run -it -d --rm \\\n",
    "--gpus '\"device=5,6\"' \\\n",
    "--name=llm_nim \\\n",
    "--shm-size=16GB  \\\n",
    "-e NGC_API_KEY \\\n",
    "-v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "-u $(id -u) \\\n",
    "-p $LLM_CONTAINER_PORT:8000 \\\n",
    "nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n",
    "\n",
    "# In order to ensure, the local NIM container is completely loaded and doesn't remain in pending stage, we instantiate a wait interval\n",
    "! sleep 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d54dcdd6-2816-4b37-89c5-33cb15c527d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 00:49:22.412 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO 11-26 00:49:22.421 api_server.py:456] Serving endpoints:\n",
      "  0.0.0.0:8000/openapi.json\n",
      "  0.0.0.0:8000/docs\n",
      "  0.0.0.0:8000/docs/oauth2-redirect\n",
      "  0.0.0.0:8000/metrics\n",
      "  0.0.0.0:8000/v1/health/ready\n",
      "  0.0.0.0:8000/v1/health/live\n",
      "  0.0.0.0:8000/v1/models\n",
      "  0.0.0.0:8000/v1/version\n",
      "  0.0.0.0:8000/v1/chat/completions\n",
      "  0.0.0.0:8000/v1/completions\n",
      "INFO 11-26 00:49:22.421 api_server.py:460] An example cURL request:\n",
      "curl -X 'POST' \\\n",
      "  'http://0.0.0.0:8000/v1/chat/completions' \\\n",
      "  -H 'accept: application/json' \\\n",
      "  -H 'Content-Type: application/json' \\\n",
      "  -d '{\n",
      "    \"model\": \"meta/llama3-8b-instruct\",\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"role\":\"user\",\n",
      "        \"content\":\"Hello! How are you?\"\n",
      "      },\n",
      "      {\n",
      "        \"role\":\"assistant\",\n",
      "        \"content\":\"Hi! I am quite well, how can I help you today?\"\n",
      "      },\n",
      "      {\n",
      "        \"role\":\"user\",\n",
      "        \"content\":\"Can you write me a song?\"\n",
      "      }\n",
      "    ],\n",
      "    \"top_p\": 1,\n",
      "    \"n\": 1,\n",
      "    \"max_tokens\": 15,\n",
      "    \"stream\": true,\n",
      "    \"frequency_penalty\": 1.0,\n",
      "    \"stop\": [\"hello\"]\n",
      "  }'\n",
      "\n",
      "INFO 11-26 00:49:22.470 server.py:82] Started server process [32]\n",
      "INFO 11-26 00:49:22.471 on.py:48] Waiting for application startup.\n",
      "INFO 11-26 00:49:22.477 on.py:62] Application startup complete.\n",
      "INFO 11-26 00:49:22.478 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "! docker logs --tail 45 llm_nim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed9719-42bf-48fb-ae7a-e2cc7c05ba69",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "WARNING 09-10 12:08:40.618 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "INFO 09-10 12:08:40.631 api_server.py:456] Serving endpoints:\n",
    "  0.0.0.0:8000/openapi.json\n",
    "  0.0.0.0:8000/docs\n",
    "  0.0.0.0:8000/docs/oauth2-redirect\n",
    "  0.0.0.0:8000/metrics\n",
    "  0.0.0.0:8000/v1/health/ready\n",
    "  0.0.0.0:8000/v1/health/live\n",
    "  0.0.0.0:8000/v1/models\n",
    "  0.0.0.0:8000/v1/version\n",
    "  0.0.0.0:8000/v1/chat/completions\n",
    "  0.0.0.0:8000/v1/completions\n",
    "INFO 09-10 12:08:40.631 api_server.py:460] An example cURL request:\n",
    "curl -X 'POST' \\\n",
    "  'http://0.0.0.0:8000/v1/chat/completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"meta/llama3-8b-instruct\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Hello! How are you?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"Hi! I am quite well, how can I help you today?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Can you write me a song?\"\n",
    "      }\n",
    "    ],\n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 15,\n",
    "    \"stream\": true,\n",
    "    \"frequency_penalty\": 1.0,\n",
    "    \"stop\": [\"hello\"]\n",
    "  }'\n",
    "\n",
    "INFO 09-10 12:08:40.681 server.py:82] Started server process [32]\n",
    "INFO 09-10 12:08:40.681 on.py:48] Waiting for application startup.\n",
    "INFO 09-10 12:08:40.710 on.py:62] Application startup complete.\n",
    "INFO 09-10 12:08:40.712 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a091e-a04e-414f-85b7-afe360291fe6",
   "metadata": {},
   "source": [
    "### Initiate A Quick Test\n",
    "You can quickly test that your NIM is up and running via two methods:\n",
    "- LangChain NVIDIA Endpoints\n",
    "- A simple OpenAI completion request\n",
    "\n",
    "**Parameter description:**\n",
    "- **base_url**: The ULR where the NIM docker image is deployed.\n",
    "- **model**: The name of the NIM model deployed. \n",
    "- **temperature**: To modulate the randomness of sampling. Reducing the temperature increases the chance of selecting words with high probabilities.\n",
    "- **top_p**: To control how deterministic the model is. If you are looking for exact and factual answers, keep this low. If you seek more diverse responses, increase to a higher value.\n",
    "- **max_tokens**: maximum number of output tokens to be generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9208de1-91a6-47aa-91f4-1ba51ca46da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tim Rosenfield is an American journalist and news anchor who has worked for several major news organizations, including CNN, MSNBC, and HLN (Headline News). He is best known for his work as a news anchor and correspondent, covering a wide range of topics including politics, business, and entertainment.\n",
      "\n",
      "Rosenfield has had a long and distinguished career in journalism, with over three decades of experience in the industry. He has worked as a news anchor and correspondent for several major networks, including CNN,\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['LLM_CONTAINER_PORT']), model=\"meta/llama3-8b-instruct\", temperature=0.1, max_tokens=100, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"Who is Tim Rosenfield?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a307f08-c431-476b-b163-e555cf142556",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immersion technology refers to a range of technologies that aim to create a more immersive and engaging experience for users, often by simulating a sense of presence or interaction with a virtual environment. Immersion technologies can be used in various fields, including entertainment, education, healthcare, and more.\n",
      "\n",
      "Some common examples of immersion technologies include:\n",
      "\n",
      "1. Virtual Reality (VR): VR uses a headset or other device to create a simulated environment that users can interact with using controllers or other devices.\n",
      "2. Augmented Reality\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['LLM_CONTAINER_PORT']), model=\"meta/llama3-8b-instruct\", temperature=0.1, max_tokens=100, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is immersion technology?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67dca88-265b-43f2-9034-5b8c9625cdf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A HyperCube is a mathematical concept that represents a higher-dimensional analog of a cube. In essence, it's a geometric shape that exists in a space with more than three dimensions.\n",
      "\n",
      "In traditional geometry, a cube is a three-dimensional shape with six square faces, each of which is a two-dimensional square. A HyperCube, on the other hand, is a shape that exists in a space with four or more dimensions. The number of dimensions in a HyperCube is typically denoted by the letter \"\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['LLM_CONTAINER_PORT']), model=\"meta/llama3-8b-instruct\", temperature=0.1, max_tokens=100, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is a HyperCube?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c952964f-dbc5-4e3a-b805-b0b4a9d6c2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMC (Social Media Company) plays a crucial role in sustainable AI (Artificial Intelligence) in several ways:\n",
      "\n",
      "1. **Data Collection and Management**: SMCs are responsible for collecting and managing vast amounts of user-generated data, which is essential for training AI models. By leveraging this data, AI systems can learn to recognize patterns, make predictions, and improve decision-making.\n",
      "2. **Data Labeling and Annotation**: SMCs can provide labeled and annotated data to AI developers, enabling them\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['LLM_CONTAINER_PORT']), model=\"meta/llama3-8b-instruct\", temperature=0.1, max_tokens=100, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is SMC role in sustainable AI?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49788a1-8a54-413c-81e9-232ac1227559",
   "metadata": {},
   "source": [
    "In case of error outputs, wait for sometime and rerun the above cell. The error might be due to the NIM container not being up completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c5b17-5d00-43fd-bbc4-c8d6c7893fcd",
   "metadata": {},
   "source": [
    "### RAG Application \n",
    "\n",
    "In this section, we will follow the steps from the previous notebook to build a RAG application that is based on the locally deployed NIM. For our demonstration, we will not create a conversational retrieval Chain using two LLMs as in the previous notebook, but a conversational retrieval chain using a single LLM `llama3-8b-instruct`. This is because each NIM image has one base model. It is possible to use the locally deployed NIM and remote access, but for clarity and ease of understanding, we will stick with a single LLM approach.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661dd09c-bdef-4907-8b78-ae22095814b9",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "921e0b4e-5477-4875-8687-8ea4e88bd0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d3c408-5735-4446-83ba-ee2c154d3fab",
   "metadata": {},
   "source": [
    "#### Create Web Link Data Source\n",
    "\n",
    "You can replace and add more web links of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e827c50-9f64-4b29-9995-405ebd0b7f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_urls = [\"https://smc.co\",\n",
    "#        \"https://smc.co/about-us\",\n",
    "#        \"https://smc.co/pricing\",\n",
    "#        \"https://smc.co/sustainability\",\n",
    "#        \"https://smc.co/future-state\"\n",
    "#       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f621c289-276d-4395-8e22-64dbfb34217f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crawling smc.co...\n",
      "Domain type: unrestricted (depth=5)\n",
      "\n",
      "Crawling firmus.co...\n",
      "Domain type: unrestricted (depth=5)\n",
      "\n",
      "Found 136 total unique links (excluding usda.gov, amd.com, washingtonpost.com):\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Set, List, Tuple\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class LinkCrawler:\n",
    "    ALLOWED_DOMAINS = {'smc.co', 'firmus.co'}  # Only these domains allow deeper crawling\n",
    "    \n",
    "    def __init__(self, base_url: str, max_depth: int = 2, delay: float = 0.1, verbose: bool = False):\n",
    "        if not base_url.startswith(('http://', 'https://')):\n",
    "            base_url = 'https://' + base_url\n",
    "            \n",
    "        self.base_url = base_url\n",
    "        self.base_domain = urlparse(base_url).netloc\n",
    "        \n",
    "        # Adjust max_depth based on domain\n",
    "        if not any(self.base_domain.endswith(allowed) for allowed in self.ALLOWED_DOMAINS):\n",
    "            self.max_depth = 1\n",
    "        else:\n",
    "            self.max_depth = max_depth\n",
    "            \n",
    "        self.visited_urls = set()\n",
    "        self.ignored_urls = set()\n",
    "        self.delay = delay\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def get_domain(self, url: str) -> str:\n",
    "        \"\"\"Extract the main domain from a URL\"\"\"\n",
    "        domain = urlparse(url).netloc\n",
    "        return domain\n",
    "    \n",
    "    def normalize_url(self, url: str) -> str:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url.lstrip('/:')\n",
    "            \n",
    "        parsed_url = urlparse(url)\n",
    "        normalized_url = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path.rstrip('/')\n",
    "        return normalized_url.split('#')[0].split('?')[0]\n",
    "    \n",
    "    def is_valid_url(self, url: str) -> bool:\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            return parsed.scheme in ['http', 'https']\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_page_links(self, url: str) -> Tuple[Set[str], Set[str]]:\n",
    "        allowed_links = set()\n",
    "        ignored_links = set()\n",
    "        \n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, timeout=10, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                absolute_url = urljoin(url, href)\n",
    "                normalized_url = self.normalize_url(absolute_url)\n",
    "                \n",
    "                if self.is_valid_url(normalized_url):\n",
    "                    if any(self.get_domain(normalized_url).endswith(allowed) for allowed in self.ALLOWED_DOMAINS):\n",
    "                        allowed_links.add(normalized_url)\n",
    "                    else:\n",
    "                        ignored_links.add(normalized_url)\n",
    "                    \n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"Error processing {url}: {e}\")\n",
    "        \n",
    "        return allowed_links, ignored_links\n",
    "    \n",
    "    def crawl(self) -> Tuple[List[str], List[str]]:\n",
    "        queue = deque([(self.normalize_url(self.base_url), 0)])\n",
    "        \n",
    "        while queue:\n",
    "            current_url, depth = queue.popleft()\n",
    "            \n",
    "            if depth >= self.max_depth:\n",
    "                continue\n",
    "                \n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(f\"Crawling: {current_url} (Depth: {depth})\")\n",
    "                \n",
    "            self.visited_urls.add(current_url)\n",
    "            allowed_links, ignored_links = self.get_page_links(current_url)\n",
    "            \n",
    "            self.ignored_urls.update(ignored_links)\n",
    "            \n",
    "            for link in allowed_links:\n",
    "                if link not in self.visited_urls:\n",
    "                    queue.append((link, depth + 1))\n",
    "        \n",
    "        return sorted(list(self.visited_urls)), sorted(list(self.ignored_urls))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_urls = [\"smc.co\", \"firmus.co\"]\n",
    "    default_max_depth = 5\n",
    "    all_allowed_urls = set()\n",
    "    all_ignored_urls = set()\n",
    "    \n",
    "    # Define domains to ignore\n",
    "    IGNORED_DOMAINS = {'usda.gov', 'washingtonpost.com', 'amd.com'}\n",
    "    \n",
    "    for target_url in target_urls:\n",
    "        try:\n",
    "            print(f\"\\nCrawling {target_url}...\")\n",
    "            crawler = LinkCrawler(target_url, max_depth=default_max_depth, verbose=False)\n",
    "            domain_type = \"unrestricted\" if any(target_url.endswith(d) for d in crawler.ALLOWED_DOMAINS) else \"restricted\"\n",
    "            print(f\"Domain type: {domain_type} (depth={crawler.max_depth})\")\n",
    "            \n",
    "            allowed_links, ignored_links = crawler.crawl()\n",
    "            all_allowed_urls.update(allowed_links)\n",
    "            all_ignored_urls.update(ignored_links)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {target_url}: {e}\")\n",
    "    \n",
    "    # Combine all URLs and filter out ignored domains\n",
    "    all_urls = {url for url in all_allowed_urls.union(all_ignored_urls) \n",
    "                if not any(urlparse(url).netloc.endswith(domain) \n",
    "                          for domain in IGNORED_DOMAINS)}\n",
    "\n",
    "    all_urls = {url for url in all_urls if not url.startswith('https://mailto:')}\n",
    "    \n",
    "    print(f\"\\nFound {len(all_urls)} total unique links (excluding {', '.join(IGNORED_DOMAINS)}):\")\n",
    "#    for url in sorted(all_urls):\n",
    "#        print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9634d7-ecfb-43b1-bb15-6a5b42261cc4",
   "metadata": {},
   "source": [
    "#### Create A Function To Load HTML Files\n",
    "\n",
    "Below is a helper function for loading html files, which we’ll use to generate the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3243f912-f387-4555-890c-788888382847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Union\n",
    "\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a document from a given URL and return it's content.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the document.\n",
    "\n",
    "    Returns:\n",
    "        The content of the document.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while making the HTTP request.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Beautiful Soup object to parse html\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get the plain text from the HTML document\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Remove excess whitespace and newlines\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae33e1-5588-4ed3-9a5b-72e26770b134",
   "metadata": {},
   "source": [
    "#### Create Embeddings and Document Text Splitter\n",
    "\n",
    "Let's create a function that initializes the path to store our embeddings, execute the `html_document_loader` function, and split the document into chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d3616d-fb9e-4e9c-b645-054348b10dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings(embeddings_model,embedding_path: str = \"./embed\"):\n",
    "\n",
    "    embedding_path = \"./embed\"\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    documents = []\n",
    "    for url in all_urls:\n",
    "#        print(f\"Working on URL {url}\")\n",
    "        document = html_document_loader(url)\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "    )\n",
    "    print(\"Total documents:\",len(documents))\n",
    "    texts = text_splitter.create_documents(documents)\n",
    "    print(\"Total texts:\",len(texts))\n",
    "    index_docs(embeddings_model,url, text_splitter, texts, embedding_path,)\n",
    "    print(\"Generated embedding successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2b731-2763-4ca9-8d51-4b99157526d2",
   "metadata": {},
   "source": [
    "#### Generate Embeddings Using NVIDIA AI Endpoints From LangChain\n",
    "\n",
    "In this section we demostrate how to generate embeddings using NVIDIA AI Endpoints for LangChain and save embeddings to offline vector store in the `/embed` directory for future re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ad47150-fbbc-4696-ad83-02f8519e8401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44ac65244729dfe096534894c131a5c7df20d79f2824c07c1bc349b453ca713b\n"
     ]
    }
   ],
   "source": [
    "! docker run -it -d --rm \\\n",
    "   --name embeddings_nim \\\n",
    "   --gpus '\"device=5,6\"' \\\n",
    "   --shm-size=16GB \\\n",
    "   -e NGC_API_KEY \\\n",
    "   -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\"  \\\n",
    "   -u $(id -u) \\\n",
    "   -p $EMBED_CONTAINER_PORT:8000 \\\n",
    "   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1\n",
    "\n",
    "! sleep 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bffd8a92-9ea3-4ced-91f7-9c67a9a38f86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|          |                                | \":\"4\"}}                        |\n",
      "|          |                                |                                |\n",
      "+----------+--------------------------------+--------------------------------+\n",
      "\n",
      "I1126 00:54:10.831496 272 server.cc:676] \n",
      "+-----------------------------------+---------+--------+\n",
      "| Model                             | Version | Status |\n",
      "+-----------------------------------+---------+--------+\n",
      "| nvidia_nv_embedqa_e5_v5           | 1       | READY  |\n",
      "| nvidia_nv_embedqa_e5_v5_model     | 1       | READY  |\n",
      "| nvidia_nv_embedqa_e5_v5_tokenizer | 1       | READY  |\n",
      "+-----------------------------------+---------+--------+\n",
      "\n",
      "I1126 00:54:10.910448 272 metrics.cc:877] \"Collecting metrics for GPU 0: NVIDIA H100 80GB HBM3\"\n",
      "I1126 00:54:10.910472 272 metrics.cc:877] \"Collecting metrics for GPU 1: NVIDIA H100 80GB HBM3\"\n",
      "I1126 00:54:10.919560 272 metrics.cc:770] \"Collecting CPU metrics\"\n",
      "I1126 00:54:10.919667 272 tritonserver.cc:2557] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.46.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data parameters statistics trace  |\n",
      "|                                  | logging                                  |\n",
      "| model_repository_path[0]         | /opt/nim/run/nim/nvidia/nv-embedqa-e5-v5 |\n",
      "|                                  | /triton-model-repository                 |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 0                                        |\n",
      "| model_config_name                |                                          |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                 |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "| cache_enabled                    | 0                                        |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1126 00:54:10.920908 272 grpc_server.cc:2463] \"Started GRPCInferenceService at 0.0.0.0:8001\"\n",
      "I1126 00:54:10.920939 272 http_server.cc:362] \"Started Metrics Service at 0.0.0.0:8002\"\n"
     ]
    }
   ],
   "source": [
    "! docker logs --tail 45 embeddings_nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "441bd1ca-1c83-45f6-b21a-f4779f5d5be8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jupyterlab/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:237: UserWarning: Default model is set as: nvidia/nv-embedqa-e5-v5. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embeddings_model = NVIDIAEmbeddings(base_url=\"http://172.17.0.1:{}/v1\".format(os.environ['EMBED_CONTAINER_PORT']))\n",
    "\n",
    "#embeddings_model = NVIDIAEmbeddings(base_url=\"http://210.87.104.19:18000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11067f62-6896-4f37-ad9f-b25fbf26bf75",
   "metadata": {},
   "source": [
    "Below, we create an `index_docs` function that loops through the document page content to extend text and metadata and applies [FAISS](https://faiss.ai/index.html). The embeddings are stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49ff7545-bb65-4864-9a77-f3536ce36178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "\n",
    "def index_docs(embeddings_model, url: Union[str, bytes], splitter, documents: List[str], dest_embed_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Split the documents into chunks and create embeddings for them.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_model: Model used for creating embeddings.\n",
    "        url: Source url for the documents.\n",
    "        splitter: Splitter used to split the documents.\n",
    "        documents: List of documents whose embeddings need to be created.\n",
    "        dest_embed_dir: Destination directory for embeddings.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for document in documents:\n",
    "        chunk_texts = splitter.split_text(document.page_content)\n",
    "        texts.extend(chunk_texts)\n",
    "        metadatas.extend([document.metadata] * len(chunk_texts))\n",
    "\n",
    "    if os.path.exists(dest_embed_dir):\n",
    "        docsearch = FAISS.load_local(\n",
    "            folder_path=dest_embed_dir, \n",
    "            embeddings=embeddings_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        docsearch.add_texts(texts, metadatas=metadatas)\n",
    "    else:\n",
    "        docsearch = FAISS.from_texts(texts, embedding=embeddings_model, metadatas=metadatas)\n",
    "\n",
    "    docsearch.save_local(folder_path=dest_embed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a4d44-800a-4083-a63e-af74ce2b04bf",
   "metadata": {},
   "source": [
    "#### Load Embeddings from the Vector Store and Build a RAG using NVIDIA Endpoints\n",
    "\n",
    "Next, we call the function `create_embeddings` and load documents from [vector store](https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/) using FAISS. The Vector store stores relevant information in a high dimensional space called embeddings.\n",
    "\n",
    "Please run the two cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0bc1104-86af-4896-894a-7de2f022fd2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing embeddings to ./embed\n",
      "Total documents: 136\n",
      "Total texts: 72498\n",
      "Generated embedding successfully\n",
      "CPU times: user 24 s, sys: 5.75 s, total: 29.7 s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "create_embeddings(embeddings_model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7920b73d-e78e-4a85-beef-5b2d22648f96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 347M\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 284M Nov 26 01:00 index.faiss\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  64M Nov 26 01:00 index.pkl\n"
     ]
    }
   ],
   "source": [
    "# load Embed documents\n",
    "! ls -lh ./embed\n",
    "\n",
    "embedding_path = \"./embed/\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embeddings_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbec25-fa14-4176-88c7-b97c6565bfb9",
   "metadata": {},
   "source": [
    "### Create A Conversational Retrieval Chain With llama3-8b-instruct\n",
    "\n",
    "Below is to query vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84565130-b12d-443d-8e5e-81928278dab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='Director of Product & AI.As our technical lead and subject matter expert in AI, DevOps, Machine Learning & Supercomputing, Dr Peter Blain’s experience stems from his PhD in Artificial Intelligence & Computer Systems Engineering, and his work as a Former Systems Architect at the Australian Integrated Marine Observing System (IMOS). Besides deep experience with Australia’s research HPC clusters, he has also developed systems and code at commercial (Biteable), research (IMOS & TPAC), and financial'), Document(metadata={}, page_content='endobj 2572 0 obj <</IsMap false/S/URI/URI(https://www.britannica.com/biography/Woodrow-Wilson)>> endobj 2573 0 obj <</IsMap false/S/URI/URI(https://www.britannica.com/topic/National-Park-Service)>> endobj 2574 0 obj <</IsMap false/S/URI/URI(https://en.wikipedia.org/wiki/Grand_Canyon_National_Park#cite_note-9)>> endobj 2575 0 obj <</Annots 2576 0 R/ArtBox[0.0 0.0 612.0 792.0]/BleedBox[0.0 0.0 612.0 792.0]/Contents 2585 0 R/CropBox[0.0 0.0 612.0 792.0]/Group 2621 0 R/MediaBox[0.0 0.0 612.0'), Document(metadata={}, page_content='Lubricant Oils to Boost Engine Efficiency with Ken Hope Hidden Horsepower by Total Seal | Podcast featuring Dr. Ken Hope Polyalphaolefins Applications Aviation and Aeronautics Synfluid® PAOs are ideal for addressing the extreme temperature differences experienced by both commercial and military aircraft. Biodegradability The biodegradability of Synfluid® PAOs addresses a critical need in many environmentally sensitive applications. Industrial Synfluid® PAO products offer lower coefficients of'), Document(metadata={}, page_content='������b��\\x13�������L��cU(\\x14 �B��(4 �B��PX(,\\x17� \\x0f \\x0f � � \\x1b���va�0( �¸pP8( \\x17� � � \\x7f�_α \\x14\\x02�p!\\\\�ɀɰɰɀɀɰɰɀɀɰɰɀɀɰɰɀɀɰɰɀɀɰɰɀɀɰɰɀɀɰɰɄɄɴɴɄɄɴɴɄɄɴɴɄɄɴɴɄɄɴɴɄɄɴɴɄɄɴɴɄɄɴɴɄɄɴɴɄɄɴɴɂɂɲɲɂɂɲɲɂɂɲɲɂɂɲɲɂɂɲɲɂɂɲɲɂɂɲɲɂ�z+�o�g�� �7��\\x16\\x06�q�� ���A�pT��]��]�3��M:ȭ\\x04 � z%\\x17\\x14P��b#���8\\x14^�B�z��c:( \\x14� ǅ@!P\\x08\\x17 D!QH\\x17҅��E�p](\\x14 �r�\\\\h\\x14\\x1a�v�]\\x18\\x14`����wL0ya���� �\\x17&�M^�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�,�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l�l� � � � � � � � � � � �'), Document(metadata={}, page_content='a greener AI: The GreenFutures Accelerator Program\",\"publishDate\":\"2024-03-25\"},{\"categories\":[{\"_rev\":\"dj4ipSn6vzKnyhGmus3XkR\",\"_type\":\"category\",\"name\":\"Interviews\",\"_id\":\"b03e4cd7-8c1e-4712-b2e3-0230bd058891\",\"_updatedAt\":\"2024-03-17T10:08:55Z\",\"slug\":{\"current\":\"Interviews\",\"_type\":\"slug\"},\"_createdAt\":\"2024-03-17T10:08:55Z\"}],\"title\":\"Director of Product & AI: Dr. Peter Blain on the Race for'), Document(metadata={}, page_content='Meet The Team: Dr. Peter Blain, Director of Product &#038; AI - SMCSustainable AI Factory →Use cases →Our Cloud →Company Updates →EnquireEnquireMenuMeet The Team: Dr. Peter Blain, Director of Product &#038; AIMeet our technical lead and subject matter expert in AI, DevOps, Machine Learning & all things Supercomputing.17 January 2023UpdatesFaceBookXLinkedInAt Firmus, we’re paving the path for a sustainable future — we’re changing the world, decarbonising one customer at a time. We work with a'), Document(metadata={}, page_content='venture aims to provide cost-effective, large-scale and sustainable GPU and AI cloud services for the AI & visual computing era.Updates17 January 2023UpdatesMeet The Team: Dr. Peter Blain, Director of Product & AIMeet our technical lead and subject matter expert in AI, DevOps, Machine Learning & all things Supercomputing.Updates16 December 2022InnovationChallenges for e-ResearchMany Australian researchers depend on access to modern accelerated computing infrastructure to remain competitive with')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "\n",
    "query = \"pete blain\"\n",
    "query_embedding = embeddings_model.embed_query(query)\n",
    "#print (query_embedding)\n",
    "\n",
    "# Perform search with the query embedding\n",
    "results = docsearch.similarity_search(query, k=7)\n",
    "\n",
    "# Output the results (the most similar documents)\n",
    "#print(\"Most relevant documents:\")\n",
    "#for result in results:\n",
    "#    print(f\"* {result.page_content} [{result.metadata}]\")\n",
    "    \n",
    "retriever = docsearch.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 7})\n",
    "response = retriever.invoke(query)\n",
    "#retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})\n",
    "\n",
    "print (response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a76ed3c7-62fe-44a0-a6b0-491e4aa7abf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['LLM_CONTAINER_PORT']),\n",
    "                 model=\"meta/llama3-8b-instruct\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_prompt=QA_PROMPT\n",
    "\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(k=7),\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f6661ae-5862-4a03-b135-d47781579ba0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62929ee2-61ee-4943-8a8f-65dc93de7a6f",
   "metadata": {},
   "source": [
    "### Test With Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ccad9fe-0b49-45fc-bd03-63146fd582d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided text does not mention Sean Zhan from Sustainable Metal Cloud.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Sean Zhan from Sustainable Metal Cloud?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c29ccf5-d674-4eed-a9fd-7c30abc8492c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, Derek Ngo is a Solutions Architect at Sustainable Metal Cloud.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Derek Ngo from Sustainable Metal Cloud?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6600762-bfc1-45c0-a184-a4a5b7ad57f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, a green data center refers to a data center that is designed to be more sustainable and energy-efficient. It appears to be a data center that uses environmentally friendly technologies and practices to reduce its carbon footprint and energy consumption.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a green data center?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d145d79b-a3c3-4132-b4e2-09c3c2cfbb40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, Firmus Immersion Technology is an advanced immersion cooling technology that allows for a significant reduction in energy consumption, making it a groundbreaking innovation in the data center industry.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is firmus immersion technology?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b85c8668-6ad5-41f8-a8c9-e8ab8eb331fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it seems that Sustainable AI refers to the development and implementation of AI technology that is environmentally sustainable and has a reduced carbon footprint. This is achieved through the use of innovative cooling technologies, such as immersion cooling, which reduce energy consumption and overall carbon emissions.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Sustainable AI?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24fa8222-85f3-4c0b-b17e-665a9ce5610b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, Sustainable Metal Cloud achieves sustainable AI through its energy-efficient technology, which has been verified by MLCommons. The company has achieved significant energy reductions, from 15 kW net power to operate to 451 kWh, and has also achieved a 7% performance improvement. Additionally, the company's technology aims to influence industry narratives and establish new benchmarks for sustainable AI practices.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does SMC achieve sustainable AI?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d866c06-7204-41a0-a309-db2333af25bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, a HyperCube appears to be a technology platform that provides scalable and flexible infrastructure for deploying artificial intelligence (AI) and machine learning workloads. It is powered by NVIDIA H100 GPUs and is designed to deliver robust performance while keeping sustainability in mind. Additionally, it is described as being highly efficient, with a Total Usage Effectiveness (TUE) of less than 1.15 in Singapore and operating within a sub 1.05 PUE envelope.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a HyperCube?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aeb235ea-5bbd-49d8-8c2d-9cc3aa8f7014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the features and capabilities of a HyperCube include:\n",
      "\n",
      "* Scalable and Flexible Infrastructure: modular design that adapts seamlessly to deploy AI at any scale\n",
      "* Uncompromised AI Power: powered by NVIDIA H100 GPUs, delivering robust performance for AI and machine learning workloads\n",
      "* Sustainable Cooling Innovation: immersion cooling technology that achieves drastic energy and CO₂ reductions\n",
      "* Adaptability and Scalability: modular design integrates seamlessly into various data center locations for cloud AZs or edge locations\n",
      "* Efficiency: operates within a sub 1.05 PUE envelope, extracts up to 30% further efficiency from the compute, and is amongst the most efficient compute platform in the world by Total Usage Effectiveness (TUE)\n"
     ]
    }
   ],
   "source": [
    "query = \"What are features and capabilities of a hypercube?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596623c-4540-4930-bd90-cd707b378ebd",
   "metadata": {},
   "source": [
    "Before we move ahead, let's free up GPU VRAM by stopping the docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fec87de1-0f2f-406d-992c-ac0bfe4b50f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_nim\n",
      "embeddings_nim\n"
     ]
    }
   ],
   "source": [
    "! docker container stop llm_nim embeddings_nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf6a5c99-c959-4de9-a2fb-1af8dd6e20b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                      COMMAND                  CREATED        STATUS        PORTS                                                                             NAMES\n"
     ]
    }
   ],
   "source": [
    "! docker ps | egrep \"^CONTAINER ID|nim\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6c74a-8437-4c27-a72a-4be6d61c8f3f",
   "metadata": {},
   "source": [
    "The next notebook walks through to add the PEFT functionalities like LoRA with NIMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "600a933d-f080-4dd9-8fdb-40559f875f24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I couldn't find any information on a person named Sean Zhan from Sustainable Metal Cloud. It's possible that the information you're looking for is not publicly available or is not well-known.\n",
      "\n",
      "If you could provide more context or details about Sustainable Metal Cloud or Sean Zhan, I may be able to help you better. Alternatively, you can try searching online or checking industry reports and publications to see if they have any information on the topic.\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['LLM_CONTAINER_PORT']), model=\"meta/llama3-8b-instruct\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "question = \"Who is Sean Zhan?\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are friendly AI!\"\n",
    "        \"Do not hallucinate.\"\n",
    "        \"Cite source of your answer.\"\n",
    "#        \"You are unhelpful and nasty AI!\"\n",
    "#        \"Your responses should be concise and no longer than two sentences.\"\n",
    "#        \"Say you don't know if you don't have this information.\"\n",
    "#        \"Say ask Jarvis if you don't know.\"\n",
    "    )),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"question\": \"Who is Sean Zhan from Sustainable Metal Cloud?\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa15b93-4154-4637-b8c6-e3a51c5b7df9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- https://developer.nvidia.com/blog/tips-for-building-a-rag-pipeline-with-nvidia-ai-langchain-ai-endpoints/\n",
    "- https://nvidia.github.io/GenerativeAIExamples/latest/notebooks/05_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html\n",
    "\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2024 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb39be-ea23-43ad-b934-2c0c188d7c01",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"rag_nim_endpoints.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"rag_nim_endpoints.ipynb\">1</a>\n",
    "        <a >2</a>\n",
    "        <a href=\"nim_lora_adapter.ipynb\">3</a>\n",
    "        <!-- <a href=\"challenge.ipynb\">4</a> -->\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"nim_lora_adapter.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../../Start-NIM-RAG.ipynb\">Home Page</a> </center> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe5f29-418b-48f7-ad13-ef5205d30a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
